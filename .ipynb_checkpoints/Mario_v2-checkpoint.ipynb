{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from tqdm import tqdm\n",
    "from nes_py.wrappers import BinarySpaceToDiscreteSpaceEnv\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import COMPLEX_MOVEMENT\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Conv2D\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img):\n",
    "    #downsample and convert to b&w\n",
    "    return cv2.resize(cv2.cvtColor(img, cv2.COLOR_RGB2GRAY),(img_width,img_height))\n",
    "\n",
    "def preprocess_states(state):\n",
    "    \n",
    "    x_t = process_image(state) #convert to b&w and downsample\n",
    "    \n",
    "    #create first state stack made of repeated first image\n",
    "    s_t = np.stack((x_t,)*frame_stack, axis=2)\n",
    "    #reshape to (1, height, width, frame_stack) for Keras\n",
    "    s_t = s_t.reshape(1,s_t.shape[0],s_t.shape[1],s_t.shape[2])\n",
    "    \n",
    "    return(s_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_CNN:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 NUM_ACTIONS, \n",
    "                 LEARNING_RATE,\n",
    "                 FRAMES_PER_ACTION,\n",
    "                 INITIAL_EPSILON,EPSILON_TAPER_LENGTH,FINAL_EPSILON,\n",
    "                 EXPERIENCE_MEMORY_LIMIT,\n",
    "                 BATCH_SIZE,\n",
    "                 GAMMA,\n",
    "                 RENDER, PROCESSED_INITIAL_STATE):\n",
    "        \n",
    "        self.NUM_ACTIONS = NUM_ACTIONS\n",
    "        self.LEARNING_RATE = LEARNING_RATE\n",
    "        self.FRAMES_PER_ACTION = FRAMES_PER_ACTION\n",
    "        self.INITIAL_EPSILON = INITIAL_EPSILON\n",
    "        self.EPSILON_TAPER_LENGTH = EPSILON_TAPER_LENGTH\n",
    "        self.FINAL_EPSILON = FINAL_EPSILON\n",
    "        self.EXPERIENCE_MEMORY_LIMIT = EXPERIENCE_MEMORY_LIMIT\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.PROCESSED_INITIAL_STATE = PROCESSED_INITIAL_STATE\n",
    "        self.RENDER = RENDER\n",
    "        self.GAMMA = GAMMA\n",
    "    \n",
    "    def construct_CNN(self):\n",
    "#  Description of how model was constructed : \n",
    "# Two hidden layers - \n",
    "# \n",
    "        model = Sequential()\n",
    "        model.add(layers.Conv2D(filters=32, kernel_size=8, strides=4, input_shape=(120, 128, 4)))\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.Conv2D(filters=64, kernel_size=2, strides=2))\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.Conv2D(filters=64, kernel_size=3, strides=1))\n",
    "        model.add(layers.Convolution2D(64, 3, 3))\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(512))\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.Dense(self.NUM_ACTIONS))\n",
    "\n",
    "        adam = Adam(lr=self.LEARNING_RATE)\n",
    "        model.compile(loss='mse', optimizer=adam)\n",
    "        \n",
    "        self.model = model\n",
    "\n",
    "    \n",
    "    def train_model(self):\n",
    "        \n",
    "        epsilon = self.INITIAL_EPSILON\n",
    "        step = 0\n",
    "        wait_steps = 100\n",
    "        s_t = self.PROCESSED_INITIAL_STATE\n",
    "        \n",
    "        while (True):\n",
    "            loss = 0\n",
    "            Q_sa = 0\n",
    "            r_t = 0\n",
    "            a_t = 0\n",
    "            if step % self.FRAMES_PER_ACTION ==0:\n",
    "\n",
    "                if random.random() <= epsilon : \n",
    "                    a_t = env.action_space.sample()\n",
    "                else:\n",
    "                    q = self.model.predict(s_t)\n",
    "                    a_t = np.argmax(q)\n",
    "\n",
    "                #Reduce epsilon: \n",
    "                if epsilon > self.FINAL_EPSILON and step > wait_steps:\n",
    "                    epsilon -= (self.INITIAL_EPSILON - self.FINAL_EPSILON)/self.EPSILON_TAPER_LENGTH\n",
    "\n",
    "                #perform selected action and get reward and next state\n",
    "                x_t1, r_t, done, info = env.step(a_t)\n",
    "                x_t1 =  process_image(x_t1) #convert to b&w and downsample\n",
    "                #add to state stack\n",
    "                x_t1 = x_t1.reshape(1, x_t1.shape[0],x_t1.shape[1], 1) #1 x height x width x 1\n",
    "                s_t1 = np.append(x_t1, s_t[:,:,:,:3], axis=3)\n",
    "\n",
    "                #store the transition in experience_memory\n",
    "                experience_memory.append((s_t, a_t, r_t, s_t1, done))\n",
    "                #clear old memories if larger than limit\n",
    "                if len(experience_memory) > self.EXPERIENCE_MEMORY_LIMIT: \n",
    "                    experience_memory.popleft()\n",
    "\n",
    "                #train if waiting period over\n",
    "                if step > wait_steps:\n",
    "                    #train on a random minibatch\n",
    "                    minibatch = random.sample(experience_memory, self.BATCH_SIZE)\n",
    "\n",
    "                    #EXPERIENCE REPLAY:\n",
    "                    state_t, action_t, reward_t, state_t1, done = zip(*minibatch)\n",
    "                    state_t = np.concatenate(state_t)\n",
    "                    state_t1 = np.concatenate(state_t1)\n",
    "                    targets = self.model.predict(state_t)\n",
    "                    Q_sa = self.model.predict(state_t1)\n",
    "                    targets[range(BATCH_SIZE), action_t] = reward_t + self.GAMMA*np.max(Q_sa, axis=1)*np.invert(done)\n",
    "\n",
    "                    loss += self.model.train_on_batch(state_t, targets)\n",
    "\n",
    "                s_t = s_t1\n",
    "                step += 1\n",
    "                if self.RENDER == True:\n",
    "                    env.render()\n",
    "\n",
    "                #save progress every 10000 iterations\n",
    "                if step % 10000 == 0: #change to 10000 after TODO\n",
    "                    model.save_weights(path+\"model.h5\", overwrite=True)\n",
    "\n",
    "                if step <= wait_steps:\n",
    "                    state = \"observe\"\n",
    "                elif step > wait_steps and step <= wait_steps + self.EPSILON_TAPER_LENGTH:\n",
    "                    state = \"exploring\"\n",
    "                else:\n",
    "                    state = \"training\"\n",
    "\n",
    "                if step % 100 == 0: \n",
    "                    print (f'TIMESTEP {step} / STATE {state} = / EPSILON {epsilon} / ACTION {a_t}' \\\n",
    "                          f'/ REWARD {r_t} / Q_MAX {np.max(Q_sa)} / LOSS {loss}')\n",
    "\n",
    "                #print(\"EPISODE FINISHED\")\n",
    "\n",
    "                #print('****************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width = 120, 128\n",
    "frame_stack = 4 #pass four frames in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 100 / STATE observe = / EPSILON 0.5 / ACTION 8/ REWARD 0 / Q_MAX 0 / LOSS 0\n",
      "TIMESTEP 200 / STATE exploring = / EPSILON 0.49835329999999844 / ACTION 4/ REWARD 0 / Q_MAX 82361.3359375 / LOSS 35750.98828125\n",
      "TIMESTEP 300 / STATE exploring = / EPSILON 0.49668996666666354 / ACTION 5/ REWARD 1 / Q_MAX 278496.03125 / LOSS 125640.453125\n",
      "TIMESTEP 400 / STATE exploring = / EPSILON 0.49502663333332864 / ACTION 5/ REWARD 1 / Q_MAX 102978.671875 / LOSS 43318.17578125\n",
      "TIMESTEP 500 / STATE exploring = / EPSILON 0.49336329999999373 / ACTION 1/ REWARD 1 / Q_MAX 17050.4140625 / LOSS 406.8846130371094\n",
      "TIMESTEP 600 / STATE exploring = / EPSILON 0.4916999666666588 / ACTION 9/ REWARD -2 / Q_MAX 4498.79345703125 / LOSS 101.24817657470703\n",
      "TIMESTEP 700 / STATE exploring = / EPSILON 0.4900366333333239 / ACTION 0/ REWARD 2 / Q_MAX 1528.9083251953125 / LOSS 19.75580596923828\n",
      "TIMESTEP 800 / STATE exploring = / EPSILON 0.488373299999989 / ACTION 9/ REWARD 2 / Q_MAX 3439.60595703125 / LOSS 33.900856018066406\n",
      "TIMESTEP 900 / STATE exploring = / EPSILON 0.4867099666666541 / ACTION 2/ REWARD 0 / Q_MAX 2759.7109375 / LOSS 91.40524291992188\n",
      "TIMESTEP 1000 / STATE exploring = / EPSILON 0.4850466333333192 / ACTION 9/ REWARD 0 / Q_MAX 2834.11279296875 / LOSS 58.5567626953125\n",
      "TIMESTEP 1100 / STATE exploring = / EPSILON 0.4833832999999843 / ACTION 5/ REWARD 0 / Q_MAX 2209.338623046875 / LOSS 69.97966003417969\n",
      "TIMESTEP 1200 / STATE exploring = / EPSILON 0.4817199666666494 / ACTION 3/ REWARD 1 / Q_MAX 1667.634521484375 / LOSS 54.476768493652344\n",
      "TIMESTEP 1300 / STATE exploring = / EPSILON 0.4800566333333145 / ACTION 0/ REWARD 0 / Q_MAX 1951.9560546875 / LOSS 60.496315002441406\n",
      "TIMESTEP 1400 / STATE exploring = / EPSILON 0.4783932999999796 / ACTION 9/ REWARD -1 / Q_MAX 1934.169189453125 / LOSS 48.676856994628906\n",
      "TIMESTEP 1500 / STATE exploring = / EPSILON 0.4767299666666447 / ACTION 4/ REWARD 0 / Q_MAX 1864.0450439453125 / LOSS 33.275821685791016\n",
      "TIMESTEP 1600 / STATE exploring = / EPSILON 0.4750666333333098 / ACTION 1/ REWARD 0 / Q_MAX 2607.076904296875 / LOSS 45.89573287963867\n",
      "TIMESTEP 1700 / STATE exploring = / EPSILON 0.4734032999999749 / ACTION 1/ REWARD 1 / Q_MAX 1877.745361328125 / LOSS 41.97844314575195\n",
      "TIMESTEP 1800 / STATE exploring = / EPSILON 0.47173996666664 / ACTION 5/ REWARD -1 / Q_MAX 2468.76806640625 / LOSS 74.75143432617188\n",
      "TIMESTEP 1900 / STATE exploring = / EPSILON 0.47007663333330507 / ACTION 9/ REWARD 0 / Q_MAX 1679.93798828125 / LOSS 22.150249481201172\n",
      "TIMESTEP 2000 / STATE exploring = / EPSILON 0.46841329999997017 / ACTION 9/ REWARD -1 / Q_MAX 2226.300048828125 / LOSS 42.71200180053711\n",
      "TIMESTEP 2100 / STATE exploring = / EPSILON 0.46674996666663526 / ACTION 9/ REWARD 0 / Q_MAX 1317.352294921875 / LOSS 15.438718795776367\n",
      "TIMESTEP 2200 / STATE exploring = / EPSILON 0.46508663333330036 / ACTION 3/ REWARD 1 / Q_MAX 1602.935546875 / LOSS 53.69966125488281\n",
      "TIMESTEP 2300 / STATE exploring = / EPSILON 0.46342329999996545 / ACTION 2/ REWARD 0 / Q_MAX 1367.021728515625 / LOSS 14.409486770629883\n",
      "TIMESTEP 2400 / STATE exploring = / EPSILON 0.46175996666663055 / ACTION 8/ REWARD -1 / Q_MAX 1424.4315185546875 / LOSS 37.372371673583984\n",
      "TIMESTEP 2500 / STATE exploring = / EPSILON 0.46009663333329565 / ACTION 1/ REWARD 1 / Q_MAX 12164.53125 / LOSS 407.54718017578125\n",
      "TIMESTEP 2600 / STATE exploring = / EPSILON 0.45843329999996074 / ACTION 2/ REWARD 0 / Q_MAX 1865.239990234375 / LOSS 17.248682022094727\n",
      "TIMESTEP 2700 / STATE exploring = / EPSILON 0.45676996666662584 / ACTION 6/ REWARD -1 / Q_MAX 1217.0845947265625 / LOSS 20.88729476928711\n",
      "TIMESTEP 2800 / STATE exploring = / EPSILON 0.45510663333329093 / ACTION 4/ REWARD 1 / Q_MAX 8530.1650390625 / LOSS 218.25271606445312\n",
      "TIMESTEP 2900 / STATE exploring = / EPSILON 0.45344329999995603 / ACTION 5/ REWARD 0 / Q_MAX 2276.4033203125 / LOSS 54.24155807495117\n",
      "TIMESTEP 3000 / STATE exploring = / EPSILON 0.4517799666666211 / ACTION 6/ REWARD 0 / Q_MAX 1677.898681640625 / LOSS 22.11859703063965\n",
      "TIMESTEP 3100 / STATE exploring = / EPSILON 0.4501166333332862 / ACTION 8/ REWARD 0 / Q_MAX 1314.90478515625 / LOSS 10.678461074829102\n",
      "TIMESTEP 3200 / STATE exploring = / EPSILON 0.4484532999999513 / ACTION 2/ REWARD 1 / Q_MAX 1065.242431640625 / LOSS 11.33840274810791\n",
      "TIMESTEP 3300 / STATE exploring = / EPSILON 0.4467899666666164 / ACTION 8/ REWARD 0 / Q_MAX 1054.2093505859375 / LOSS 12.252145767211914\n",
      "TIMESTEP 3400 / STATE exploring = / EPSILON 0.4451266333332815 / ACTION 7/ REWARD 2 / Q_MAX 952.5912475585938 / LOSS 12.347884178161621\n",
      "TIMESTEP 3500 / STATE exploring = / EPSILON 0.4434632999999466 / ACTION 9/ REWARD -1 / Q_MAX 1161.0328369140625 / LOSS 18.027420043945312\n",
      "TIMESTEP 3600 / STATE exploring = / EPSILON 0.4417999666666117 / ACTION 4/ REWARD 1 / Q_MAX 907.4248046875 / LOSS 9.385090827941895\n",
      "TIMESTEP 3700 / STATE exploring = / EPSILON 0.4401366333332768 / ACTION 3/ REWARD 0 / Q_MAX 831.9686889648438 / LOSS 4.382948875427246\n",
      "TIMESTEP 3800 / STATE exploring = / EPSILON 0.4384732999999419 / ACTION 1/ REWARD 0 / Q_MAX 835.6301879882812 / LOSS 9.820271492004395\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-4c8ed04ba650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_CNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mCNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-77-4468edbf8dd7>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0mstate_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0mstate_t1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                     \u001b[0mQ_sa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_t\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGAMMA\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_sa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1491\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m       return training_arrays.predict_loop(\n\u001b[0;32m-> 1493\u001b[0;31m           self, x, batch_size=batch_size, verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, inputs, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2912\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2914\u001b[0;31m     \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2915\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2916\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = BinarySpaceToDiscreteSpaceEnv(env, COMPLEX_MOVEMENT)\n",
    "\n",
    "env.reset()\n",
    "experience_memory = deque()\n",
    "x_t, reward, done, info = env.step(env.action_space.sample())\n",
    "s_t = preprocess_states(x_t)\n",
    "\n",
    "CNN = Q_CNN(NUM_ACTIONS = len(COMPLEX_MOVEMENT),\n",
    "              LEARNING_RATE = 1e-4,\n",
    "              FRAMES_PER_ACTION = 1,\n",
    "              INITIAL_EPSILON = 0.5, EPSILON_TAPER_LENGTH = 30000,FINAL_EPSILON = 0.001,\n",
    "              EXPERIENCE_MEMORY_LIMIT = 50000,\n",
    "              BATCH_SIZE = 32,\n",
    "              GAMMA = 0.99,\n",
    "              RENDER = True,\n",
    "              PROCESSED_INITIAL_STATE = s_t )\n",
    "\n",
    "CNN.construct_CNN()\n",
    "CNN = CNN.train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
